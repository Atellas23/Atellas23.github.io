I"ôô<h1 id="machine-learning-1">Machine Learning 1</h1>

<h2 id="1-introduction-to-machine-learning">1. Introduction to Machine Learning</h2>

<p>Machine learning is a field that lies at the intersection of statistics, probability, computer science, and optimization. The main goal is to explore <strong>automatic methods for inferring models from data</strong> (for example: finding structure, making predictions).</p>

<p><span style="color:green">Examples of learning tasks:</span></p>

<ul>
  <li><span style="color:brown">SUPERVISED LEARNING:</span> uses labeled data.
    <ul>
      <li><strong>Classification:</strong> predicting a class or category to each example; note multi-label, probabilistic generalizations.</li>
      <li><strong>Regression:</strong> predicting a real value for each example; note multi-variable generalization.</li>
    </ul>
  </li>
  <li><span style="color:brown">UNSUPERVISED LEARNING:</span> does not use or have data labels.
    <ul>
      <li><strong>Clustering:</strong> discovering homogeneous groups (clusters) in data.</li>
      <li><strong>Dimensionality reduction:</strong> finding lower-dimensional data representations.</li>
      <li><strong>Density estimation:</strong> estimating the probabilistic mechanism that generates data.</li>
      <li><strong>Novelty detection:</strong> finding anomalous/novel/outlying data.</li>
    </ul>
  </li>
  <li><span style="color:brown">SEMI-SUPERVISED LEARNING:</span> uses partly labeled data.
    <ul>
      <li><strong>Ranking:</strong> ordering examples according to some criterion.</li>
      <li><strong>Reinforcement:</strong> delayed rewarding.</li>
    </ul>
  </li>
  <li><span style="color:brown">TRANSFER LEARNING:</span> learning in a new task through the transfer of knowledge from a related task that has already been learned.</li>
</ul>

<h3 id="useful-probability-and-statistics-facts">Useful probability and statistics facts</h3>

<ul>
  <li>
    <p><strong><span style="color:brown">Central Limit Theorem</span>:</strong></p>

    <p>If $X_1,\ldots,X_n$ are <em>independent identically distributed random variables</em>, with $\mathbb E[X_i]=\mu$ and $\text{Var}(X_i)=\sigma^2$, then the sample mean</p>
  </li>
</ul>

<script type="math/tex; mode=display">\frac{X_1+\ldots+X_n}{n}\sim\mathcal N\left(\mu,\frac{\sigma^2}{n}\right)</script>

<p>‚Äã		approaches a normal distribution as $n\rightarrow\infty$.</p>

<ul>
  <li>
    <p><strong><span style="color:brown">Product rule</span>:</strong></p>

    <p>If $X_1,\ldots,X_n$ have a joint probability distribution $p(X_1,\ldots,X_n)$, then we can factorize the distribution as the product</p>
  </li>
</ul>

<script type="math/tex; mode=display">p(X_1,\ldots,X_n)=p(X_1)\prod_{i=2}^np(X_i|X_1,\ldots,X_{i-1}).</script>

<ul>
  <li><strong><span style="color:brown">Bayes Theorem</span>:</strong></li>
</ul>

<script type="math/tex; mode=display">P(B_i|A)=\frac{P(A|B_i)P(B_i)}{\sum_jP(A|B_j)P(B_j)}=\frac{P(A|B_i)P(B_i)}{P(A)}.</script>

<ul>
  <li>
    <p><strong><span style="color:brown">Bayes formula for densities</span>:</strong></p>

    <p>In a data analysis context, $\theta$ is a parameter vector and the following equality holds:</p>
  </li>
</ul>

<script type="math/tex; mode=display">\pi_{\text{POST}}(\theta|\text{data})=\frac{\pi_{\text{LIK}}(\text{data}|\theta)\cdot\pi_{\text{PRIOR}}(\theta)}{\int_{\Theta}\pi_{\text{LIK}}(\text{data}|\theta)\cdot\pi_{\text{PRIOR}}(\theta)d\theta}.</script>

<p>‚Äã		This can also be expressed loosely as
<script type="math/tex">P(\theta|D)=\frac{P(D|\theta)P(\theta)}{P(D)}=\frac{P(D|\theta)P(\theta)}{\int_\Theta P(D|\theta)P(\theta)d\theta},</script>
‚Äã		where $D$ is the data. This expression gives rise to the notions of <strong>likelihood</strong>, <strong>prior</strong>, <strong>posterior</strong>, and <strong>unconditional (expected likelihood)</strong> distributions:</p>

<ul>
  <li>$P(\theta)$: <u>prior probability</u>, confidence in $\theta$ before observing $D$.</li>
  <li>$P(D\vert \theta)$: <u>likelihood</u>, probability of observing $D$ if parameters are $\theta$.</li>
  <li>$P(D)$: <u>expected likelihood</u> of observing data $D$, also <u>unconditional</u>.</li>
  <li>
    <p>$P(\theta\vert  D)$: <u>posterior probability</u>, confidence in $\theta$ after observing $D$.</p>
  </li>
  <li>
    <p><strong><span style="color:brown">Conjugacy</span>:</strong></p>

    <p><strong>Definition:</strong> Suppose a prior distribution $\pi_{\text{PRIOR}}(\theta)$ belongs to a class of parametrized distributions $\Pi$. Then the distribution is said to be <strong>conjugate</strong> with respect to a likelihood $\pi_{\text{LIK}}(\cdot\vert \theta)$ if the posterior distribution $\pi_{\text{POST}}(\theta\vert \cdot)\in\Pi$.</p>

    <p>Remember that $\pi_{\text{POST}}(\theta\vert \cdot)\propto\pi_{\text{LIK}}(\cdot\vert \theta)\pi_{\text{PRIOR}}(\theta).$ For example, Gaussian is conjugate to Gaussian, and Beta is conjugate to Binomial.</p>
  </li>
</ul>

<p><span style="color:green">Using the posterior:</span>
<script type="math/tex">\hat\theta_{\text{MAP}}:=\text{argmax}_{\theta\in\Theta}\{P(\theta|D)\}\text{: the value of }\theta\text{ that maximizes the posterior.}
\\
\hat\theta_{\text{ML}}:=\text{argmax}_{\theta\in\Theta}\{P(D|\theta)\}\text{: the value of }\theta\text{ that maximizes the likelihood.}
\\
\hat\theta_{\text{EV}}:=\mathbb{E}[P(\theta|D)]=\int_{\Theta}P(\theta|D)\cdot P(\theta)d\theta\text{: the expected value of theta.}</script></p>

<h3 id="inductive-bias">Inductive bias</h3>

<p><strong><span style="color:blue">Example:</span></strong> complete the following series: $2,4,6,8,‚Ä¶$</p>

<p><strong>Answer 1:</strong> 132 (model 1: $f(n)=n^4-10n^3+35n^2-48n+24$)</p>

<p><strong>Answer 2:</strong> 10 (model 2: $f(n)=2n$)</p>

<p>How can we rule out the more complex model?</p>

<ol>
  <li>
    <p>Supply more training data: $2,4,6,8,10,12,14,‚Ä¶$</p>
  </li>
  <li>
    <p>Regularize: add penalty to higher-order terms.</p>
  </li>
  <li>
    <p>Reduce the hypothesis space; for example, restrict to quadratic models.
<script type="math/tex">\text{Class of functions: }\mathcal F:=\{f_\theta:\mathcal X\rightarrow\mathcal Y\ |\ \theta\in\Theta\}.</script></p>
  </li>
</ol>

<p>So, the conclusions are this: based <em>only</em> on training data $D$, there is no means of choosing which function $f$ is better (generalitzation is not <em>guaranteed</em>). Thus, we must add control to the <strong>fitting ability</strong> of our methods (complexity control).
<script type="math/tex">\text{true error}(f)\leq\text{training error}(f)+\text{complexity of }f.</script></p>

<h3 id="formulation-of-ml">Formulation of ML</h3>

<p><img src="C:\Users\alexb\AppData\mathbb{R}oaming\Typora\typora-user-images\image-20200214212648087.png" alt="image-20200214212648087" /></p>

<p>$X$ are the measured variables, $Z$ are the unmeasured ones, $y$ is the true function and $y‚Äô$, which would be $\hat y$, is the modeled function. There are some important translations from Statistics to ML and vice-versa:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Machine Learning</th>
      <th style="text-align: center">Statistics</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">model</td>
      <td style="text-align: center">model</td>
    </tr>
    <tr>
      <td style="text-align: center">parameter/weight</td>
      <td style="text-align: center">parameter/coefficient</td>
    </tr>
    <tr>
      <td style="text-align: center">train</td>
      <td style="text-align: center">fit</td>
    </tr>
    <tr>
      <td style="text-align: center">learn</td>
      <td style="text-align: center">infer/estimate</td>
    </tr>
    <tr>
      <td style="text-align: center">regression</td>
      <td style="text-align: center">regression</td>
    </tr>
    <tr>
      <td style="text-align: center">classification</td>
      <td style="text-align: center">discrimination</td>
    </tr>
    <tr>
      <td style="text-align: center">clustering</td>
      <td style="text-align: center">clustering/classification</td>
    </tr>
    <tr>
      <td style="text-align: center">inputs/features/variables</td>
      <td style="text-align: center">independent variables, explanatory variables, predictors</td>
    </tr>
    <tr>
      <td style="text-align: center">ouputs/targets</td>
      <td style="text-align: center">dependent/response variables</td>
    </tr>
    <tr>
      <td style="text-align: center">instances/examples</td>
      <td style="text-align: center">individuals/observatiobs</td>
    </tr>
    <tr>
      <td style="text-align: center">error/loss function, training/empirical error, true/generalization error</td>
      <td style="text-align: center">fit criterion, deviance resubstitution/in-sample error, predictive, out-sample error</td>
    </tr>
  </tbody>
</table>

<h4 id="prediction-vs-inference">Prediction vs. inference</h4>

<p><strong>Prediction:</strong> produce a good estimate for the predicted variable.</p>

<p><strong>Inference:</strong></p>

<ol>
  <li>Which predictors actually affect the target variable?</li>
  <li>How strong are these dependencies?</li>
  <li>Are these relationships positive or negative?</li>
</ol>

<h3 id="common-tasks">Common Tasks</h3>

<ul>
  <li>
    <p><strong>Regression:</strong> The goal is to predict some quantitative outcome subject to probabilistic uncertainty.</p>
  </li>
  <li>
    <p><strong>Classification:</strong> The goal is to obtain a model based on available <strong>training data</strong> (<em>known</em> examples) with high classification accuracy on  unseen <em>unknown</em> examples (<strong>test data</strong>), i.e. achieving good <strong>generalization</strong>.</p>
  </li>
  <li>
    <p><strong>Clustering:</strong> The goal is to find homogeneous groups of data and set them apart accordingly. Looks like a very different task from regression or classification, but it‚Äôs both of them with some added difficulty: it has an inherent large subjectivity.</p>
  </li>
</ul>

<p><strong>Why are these tasks stochastic?</strong></p>

<p>We have a (complete) input data object $(x,z)$ and an output data object $y$. The true relationship is $f_c:\mathcal X\times\mathcal Z\rightarrow\mathcal Y$, that is $f_c(x,z)=y$. When we measure data about $f_c$, we measure only the $x$ portion of the input variables. Therefore, the relation between $x$ and $y$ becomes stochastic.</p>

<h4 id="setting-up-the-tasks">Setting up the tasks</h4>

<p>There are (at least) two ways of setting up these tasks formally:</p>

<h5 id="optimization-view">Optimization view</h5>

<script type="math/tex; mode=display">\min_{\theta\in\Theta}E(\theta):=\frac{1}{n}\sum_{i=1}^nl(y_i,f_\theta(x_i))+\Omega(f_\theta).</script>

<p>true error of $f_\theta\leq$ training error of $f_\theta$ + complexity of $f_\theta$ (empirical risk + regularizer)</p>

<p>$l(y_i,f_\theta(x_i))$ is called the <em>loss/error function</em>.</p>

<h5 id="statistics-view">Statistics view</h5>

<p>Use Bayes‚Äô formula to compute $P(\theta\vert \text{data})$ and choose one according to this (posterior) distribution.</p>

<p>Many times these two views can yield the same results (which is good!). An example would be LSQ $\equiv$ MaxLik+Gaussian.</p>

<p>The most general description of the data generation mechanism is in terms of the pdf $p(x,y)$ in the joint input-output space: this is the key to generalization.
<script type="math/tex">p(x,y)=p(y|x)\cdot p(x),\text{ where }p(x)=\int p(y,x)\ dy.</script>
Some techniques use $p(x)$, others do not. The important pdf is $p(y\vert x)$. <em>Discriminative</em> methods use only $p(y\vert x)$, while <em>generative</em> methods use the joint pdf $p(x,y)$.</p>

<p><strong>So, what is a Machine Learning algorithm/technique?</strong></p>

<p>A ML algorithm gets a dataset $D$ and returns a model of $D$ ( a representation of $D$ that either gives structure to $D$ or that allows to make predictions on unseen observations), together with an estimation of the model quality. The algorithm itself typically determines the model space $\mathcal F$ and the loss function $l$.</p>

<p><strong>And why are linear models so nice?</strong></p>

<p>We will begin our analyses with linear models and techniques. A model is linear when, up to an invertible mapping, it is <strong>a linear function of its parameters</strong>; $f_\theta(x)$ is linear when is depends linearly on $\theta$, but we do not say anything about $x$. For example, $f_\theta(x)=\sum_{i=0}^m\theta_i\sin(\exp(-x_i^2))$ is linear. A linear model:</p>

<ul>
  <li><strong>Is analytically tractable:</strong> we have closed-form solutions or fast convergent iterative methods for the solution.</li>
  <li><strong>Has a unique solution:</strong> there are no local optima.</li>
  <li><strong>Is highly interpretable.</strong></li>
  <li><strong>Is amenable to inference:</strong> we can ask (and answer) questions about the importance and weight on the target of the different variables.</li>
  <li>Has <strong>user-defined fitting ability</strong>, via the basis functions.</li>
  <li>Is capable of being <strong>regularized:</strong> complicated models can be penalized.</li>
</ul>

<h4 id="general-form-of-a-linear-model">General form of a linear model</h4>

<p>A linear model has a general expression as
<script type="math/tex">f(x;\theta)=g\left(\theta_0+\sum_{i=1}^h\theta_i\varphi_i(x)\right).</script>
The functions $\varphi_i$ are called <strong>basis functions</strong> (they constitute a <em>feature map</em>) and are non-linear wrt $x$. $g$ is a strictly monotonic function: in Neural Networks, this is called an <strong>activation function</strong>.</p>

<h3 id="on-data-pre-processing">On data pre-processing</h3>

<p>Each problem requires a different approach in what concerns data cleaning and preparation. This pre-processing procedure is very important because it can have a deep impact on performance; it can easily take us a significant part of the time. So, the important things to take into account on data pre-processing are:</p>

<ul>
  <li>Treatment of missing, anomalous, and incoherent or incorrect values.</li>
  <li>Coding of non-continuous or non-ordered variables.</li>
  <li>Possible elimination of irrelevant or redundant variables (<em>feature selection</em>).</li>
  <li>Creation of new variables that can be useful (<em>feature extraction</em>).</li>
  <li>Normalization of the variables (standardization).</li>
  <li>Transformations of the variables (for example, corrections of serious skewness and/or kurtosis)</li>
</ul>

<p>Non-standard data (images, audio, text‚Ä¶) may need completely <em>ad hoc</em> treatments.</p>

<h2 id="2-linear-data-visualization">2. Linear Data Visualization</h2>

<h3 id="dimensionality-reduction">Dimensionality reduction</h3>

<p>There are two main goals associated to these techniques:</p>

<ul>
  <li><strong>Signal representation:</strong> the goal is to represent the data accurately in a lower-dimensional space.</li>
  <li><strong>Signal classification:</strong> the goal is to enhance the class-discriminatory information in the lower-dimensional space.</li>
</ul>

<p>Unfortunately, there is no systematic way to generate non-linear transforms, so we will focus on <strong>linear</strong> methods for <strong>feature extraction</strong>:</p>

<ul>
  <li><span style="color:brown">PCA</span>: Principal Components Analysis.</li>
  <li><span style="color:brown">FDA/LDA</span>: Fisher‚Äôs Discriminant Analysis.</li>
  <li><span style="color:brown">ICA</span>: Independent Components Analysis.</li>
</ul>

<h3 id="principal-components-analysis">Principal Components Analysis</h3>

<p><span style="color:red"><strong>ho farem a AD en principi</strong></span></p>

<h3 id="fishers-discriminant-analysis">Fisher‚Äôs Discriminant Analysis</h3>

<p>FDA is a technique for <strong>dimensionality reduction, supervised classification, feature extraction and data visualization</strong>.</p>

<p><strong><span style="color:blue">Idea:</span></strong> projection of the data onto a lower dimensional linear space, such that the separability of projected data is maximized.</p>

<p>Fisher‚Äôs idea is to regard <strong>dot product</strong> as the projection $y$ of some $x\in\mathbb{R}^p$ from classes $\omega_1$ or $\omega_2$, via a projection vector $w$: $y=w^Tx\in\mathbb{R}$. In order to find a good projection vector, we need to define a measure of separation between the projections:
<script type="math/tex">m_k=\frac{1}{n_k}\sum_{i\in\omega_k}x_i,\qquad k\in\{1,2\},</script>
where $n_1+n_2=n$ is the number of examples on every class. We then choose to maximize the <em>squared</em> distance between the projected means,
<script type="math/tex">(\mu_2-\mu_1)^2=(w^Tm_2-w^Tm_1)^2=(w^T(m_2-m_1))^2.</script>
However, the distance between the projected means is not a very good measure since it does not take into account the dispersion (<strong>scatter</strong>) within the classes. The problem is that the covariance matrices for each class are far from being diagonal. We actually want to look for the projection where examples from the same class are projected very close to one another and the projected means are as far apart as possible:</p>

<p><img src="ML1.assets/image-20200220172900080.png" alt="image-20200220172900080" style="zoom:50%;" /></p>

<p>The solution (proposed by R. Fisher) is to maximize a function that represents the difference between the means, normalized by a measure of the within-class scatter:</p>

<ol>
  <li>$\forall k$ a class we define the scatter as</li>
</ol>

<script type="math/tex; mode=display">s_k^2=\sum_{i\in\omega_k}\left(w^Tx_i-\mu_k\right)^2,\qquad k\in\{1,2\}.</script>

<ol>
  <li>The total scatter is $s_1^2+s_2^2$.</li>
  <li>Fisher‚Äôs idea was to maximize the following function:</li>
</ol>

<script type="math/tex; mode=display">J(w)=\frac{(\mu_2-\mu_1)^2}{s_1^2+s_2^2}.</script>

<p>It can be shown that $J(w)$ can be rewritten as:
<script type="math/tex">J(w)=\frac{(\mu_2-\mu_1)^2}{s_1^2+s_2^2}=\frac{w^TS_Bw}{w^TS_Ws},</script>
where</p>

<ul>
  <li>$S_B=(m_2-m_1)(m_2-m_1)^T$ is the <strong>between-class scatter matrix</strong> (rank 1).</li>
  <li>$S_W=\sum_{i\in\omega_1}(x_1-m_1)(x_1-m_1)^T+\sum_{i\in\omega_2}(x_1-m_2)(x_1-m_2)^T$ is the <strong>within-class scatter matrix</strong>.</li>
</ul>

<p>To find the maximum of $J$ we derive and equal to zero,
<script type="math/tex">\frac{\partial J}{\partial w}=0,</script>
and upon solving we arrive at the following <strong>generalized eigenvalue problem</strong>:
<script type="math/tex">\left(S_W^{-1}S_B\right)w=J(w)w;</script>
solving it yields $\boxed{\hat w=S_W^{-1}(m_1-m_2),}$ known as <strong>Fisher‚Äôs Linear Discriminant</strong> (1936), although it is not a discriminant but a specific choice for projection down to one dimension.</p>

<p>FDA generalizes very gracefully for $K$ class problems: the only restriction is that the maximum number of projection directions is $K-1$. FDA can also be derived as the Maximum Likelihood result for the case of Gaussian class-conditional densities with equal covariance matrices; in this case, it is known as LDA.</p>

<p><strong><em>WARNING!</em></strong> FDA is able to extract a maximum of $K-1$ projection directions, maybe insufficient for complex data. PCA is able to extract $d$ projection directions, but it is not clear how many are necessary.</p>

<h4 id="counterparts">Counterparts</h4>

<p><strong>When will FDA presumably fail?</strong> If the classes are far from Gaussian, the FDA projections will not be able to preserve any complex structure; for an example, this image:</p>

<p><img src="ML1.assets/image-20200220171928773.png" alt="image-20200220171928773" style="zoom:70%;" /></p>

<p>FDA will also fail when the discriminatory information is not in the mean but rather in the <em>variance</em> of the data (e.g., if $J(w)=0$); for example,</p>

<p><img src="ML1.assets/image-20200220172136456.png" alt="image-20200220172136456" style="zoom:50%;" /></p>

<h2 id="3-theory-for-regression-and-linear-models-i">3. Theory for regression and linear models (I).</h2>

<h3 id="the-regression-framework">The regression framework</h3>

<p>Given data $D={(x_n,t_n)}_{n=1,\ldots,N}$, where $x_n\in\mathbb{R}^d,t_n\in\mathbb{R}$,</p>

<ul>
  <li><strong>Statistics:</strong> estimation of a continuous random variable $T$ conditioned on a random vector $X$.</li>
  <li><strong>Mathematics:</strong> estimation of a real function $f$ based on a finite number of <em>noisy</em> examples $(x_n,f(x_n))$.</li>
</ul>

<p>The departing statistical setting is $t_n=f(x_n)+\varepsilon_n$; a model is any approximation of $f$. We assume $\varepsilon_n$ are iid random variables such that $\mathbb E[\varepsilon_n]=0$ and $\text{Var}(\varepsilon_n)=\sigma^2&lt;\infty$, and that $\varepsilon_n$ and $x_n$ are independent variables.</p>

<p>The <strong>risk</strong> of a model $y$ is
<script type="math/tex">R(y):=\int_\R\int_{\R^d}L(t,y(\boldsymbol{x}))p(t,\boldsymbol{x})\ d\boldsymbol{x}\ dt,</script>
where $L$ is a suitable <strong>loss</strong> function that satisfies:</p>

<ul>
  <li>$L(t,y(\boldsymbol x))\geq0$</li>
  <li>$L(t,y(\boldsymbol x))=0\impliedby t=y(\boldsymbol x)$ (not necessarily in the other direction)</li>
  <li>$L(t,y(\boldsymbol x))$ does not increase when $\vert  t-y(\boldsymbol x)\vert $ decreases.</li>
</ul>

<p>$L$ is closely related to the distribution of the noise model $\varepsilon_n$.</p>

<p><span style="color:blue"><strong>Example:</strong></span> if we assume for example that $\varepsilon_n\sim\mathcal N\left(0,\sigma^2\right)$, using a maximum likelihood argument it can be shown that the <em>right</em> loss function is the <strong>square error</strong>:
<script type="math/tex">L_{\text{SE}}(t,y(\boldsymbol x)):=(t-y(\boldsymbol x))^2.</script>
The <strong>risk</strong> is therefore
<script type="math/tex">R(y)=\int_\R\int_{\R^d}(t-y(\boldsymbol x))^2p(t|\boldsymbol x)p(\boldsymbol x)\ d\boldsymbol x\ dt</script>
If we enjoy complete freedom to choose $y$, the solution is:
<script type="math/tex">y^*(\boldsymbol x)=\int_\R tp(t|\boldsymbol x)\ dt=f(\boldsymbol x),</script>
known as the <strong>regression function</strong>. Since $\mathbb E[\varepsilon_n]=0$, we can alternatively express the regression setting by stating that $t$ is a continuous random variable such that $f(\boldsymbol x)=\mathbb E[t\ \vert \ X=\boldsymbol x]$.</p>

<p><strong>Claim:</strong> $y^*(\boldsymbol x)=f(\boldsymbol x).$</p>

<p><strong>Proof:</strong>
<script type="math/tex">y^*(\boldsymbol x)=\mathbb E[t\ |\ X=\boldsymbol x]=\mathbb E[f(\boldsymbol x)+\varepsilon\ |\ X=\boldsymbol x]=\\=\mathbb E[f(\boldsymbol x)\ |\ X=\boldsymbol x]+\mathbb E[\varepsilon\ |\ X=\boldsymbol x]=f(\boldsymbol x)+0=\\=f(\boldsymbol x).\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\ \square</script>
In a practical setting, we don‚Äôt know $p(t\vert \boldsymbol x)$. Instead, we have a finite i.i.d. <strong>data sample</strong> of $N$ labeled observations $D={(\boldsymbol x_n,t_n)}_{n=1,\ldots,N}$, where $\boldsymbol x_n\in\mathbb{R}^d,t_n\in\mathbb{R}$. Then, <strong>intuition</strong> tells us to solve for $y$
<script type="math/tex">\min_y\int_{\R^d}(f(\boldsymbol x)-y(\boldsymbol x))^2p(\boldsymbol x)\ d\boldsymbol x.</script>
This is equivalent to minimizing the risk function; we‚Äôll see this in the next part. For now, we must impose restrictions on the possible solutions $y$, this is, we must restrict the search space to a specific <strong>class of functions</strong> $\mathcal Y$.</p>

<p>We can compute an approximation to the true risk, called the <strong>empirical risk</strong>, by averaging the loss function on the available data $D$:
<script type="math/tex">R_{\text{emp}}(y):=\frac{1}{N}\sum_{n=1}^N(t_n-y(\boldsymbol x_n))^2.</script>
This quantity is also known as the <strong>apparent error</strong>. The <strong>Empirical Risk Minimization (ERM)</strong> principle stats that a learning algorithm should choose a hypothesis (model) $\hat y$ which minimizes the empirical risk among a predefined class of functions $\mathcal Y$:
<script type="math/tex">\hat y:=\arg\min_{y\in\mathcal Y}R_{\text{emp}}(y).</script>
The quantity $R_{\text{emp}}(\hat y)$ is known as the <strong>training error</strong>. In theoretical ML, we are very much interested in:</p>

<ul>
  <li>How this error fluctuates as a function of the data $D$.</li>
  <li>How far this error is from the true error, this is, to bound $\vert R_{\text{emp}}(\hat y)-R(y)\vert $; at the very least, to bound $\vert \mathbb E[R_{\text{emp}}(\hat y)]-R(y)\vert $.</li>
  <li>How far this error is from the best possible error, this is, to bound $\vert R_{\text{emp}}(\hat y)-R(y^<em>)\vert $; at the very least, to bound $\vert \mathbb E[R_{\text{emp}}(\hat y)]-R(y^</em>)\vert $.</li>
</ul>

<h3 id="bias-variance-analysis">Bias-Variance analysis</h3>

<p>Recall the assumption that $\varepsilon_n\sim\mathcal N(0,\sigma^2)$. In this case, using the square error, the risk can be decomposed as:
<script type="math/tex">R(y)=\int_\R\int_{\R^d}(t-y(\boldsymbol x))^2p(t,\boldsymbol x)\ d\boldsymbol x\ dt=\int_\R\int_{\R^d}(t-f(\boldsymbol x))^2p(t,\boldsymbol x)\ d\boldsymbol x\ dt\\+\int_{\R^d}(f(\boldsymbol x)-y(\boldsymbol x))^2p(\boldsymbol x)\ d\boldsymbol x=\sigma^2+\int_{\R^d}(f(\boldsymbol x)-y(\boldsymbol x))^2p(\boldsymbol x)\ d\boldsymbol x=\boxed{\sigma^2+\text{MSE}(y),}</script>
where $f$ is the <strong>regression function</strong>. Therefore, we arrive at $R(y)=\sigma^2+\text{MSE}(y)$. We can now <em>forget</em> about $\sigma^2$ and the risk and instead aim at minimizing the $\text{MSE}(y)$:
<script type="math/tex">\text{MSE}(y)=\int_{\R^d}(f(\boldsymbol x)-y(\boldsymbol x))^2p(\boldsymbol x)\ d\boldsymbol x.</script>
A <strong>learning algorithm</strong> for <strong>regression</strong> is a procedure that, given data $D$ and the search space $\mathcal Y$, outputs a model $y_D\in\mathcal Y$ that aims at minimizing $\text{MSE}(y)$.</p>

<p>Consider now one particular $\boldsymbol x_0$; different $D$ will produce different $y_D$ and therefore different predictions $y_D(\boldsymbol x_0)$. Let us concentrate on the quantity $(f(\boldsymbol x_0)-y(\boldsymbol x_0))^2$: we wish to eliminate the dependence on $D$. Therefore, we investigate its expected value, $\mathbb E\left[(f(\boldsymbol x_0)-y(\boldsymbol x_0))^2\right]$, taking over all possible $D$ of size $N$. If we develop a little more their formulas,
<script type="math/tex">\mathbb E\left[(f(\boldsymbol x_0)-y(\boldsymbol x_0))^2\right]=(f(\boldsymbol x_0)-\mathbb E[y_D(\boldsymbol x_0)])^2+\mathbb E\left[(y_D(\boldsymbol x_0)-\mathbb E[y_D(\boldsymbol x_0)])^2\right].</script>
We can interpret these summands as $f(\boldsymbol x_0)-\mathbb E[y_D(\boldsymbol x_0)]=\text{Bias}(y_D(\boldsymbol x_0))$, and $\mathbb E\left[(y_D(\boldsymbol x_0)-\mathbb E[y_D(\boldsymbol x_0)])^2\right]=\text{Var}(y_D(\boldsymbol x_0))$. Then, the formula is more clearly stated as
<script type="math/tex">\text{MSE}(y_D(\boldsymbol x_0))=\text{Bias}^2(y_D(\boldsymbol x_0))+\text{Var}(y_D(\boldsymbol x_0)),</script>
and the <strong>risk</strong> can be expressed as a sum of three summands:
<script type="math/tex">R(y_D(\boldsymbol x_0))=\sigma^2+\text{Bias}^2(y_D(\boldsymbol x_0))+\text{Var}(y_D(\boldsymbol x_0)).</script>
The derivation above depends on a particular point $\boldsymbol x_0$, so let us put it all back in place within their integrals:
<script type="math/tex">\text{Bias}^2(y_D)=\int_{\R^d}\text{Bias}^2(y_D(\boldsymbol x))p(\boldsymbol x)\ d\boldsymbol x,\\
\text{Var}(y_D)=\int_{\R^d}\text{Var}(y_D(\boldsymbol x))p(\boldsymbol x)\ d\boldsymbol x,\\
\boxed{R(y_D)=\sigma^2+\text{Bias}^2(y_D)+\text{Var}(y_D).}</script></p>

<p>In general, an <strong>underfit</strong> model will have a big bias, while an <strong>overfit</strong> model will have a high variance. The <em>abilityi to fit</em> has a name: it‚Äôs called the <strong>complexity</strong> of the function class. Both models that are more or less complex than needed will tend to have large prediction errors. In the former, this will be dominated by the variance term, while in the latter, it will be dominated by the (square) bias term.</p>

<h2 id="4-regression-theory-and-linear-regression-models-ii">4. Regression theory and linear regression models (II)</h2>

<p>Our departing statistical model still is
<script type="math/tex">t_n=f(\boldsymbol x_n)+\varepsilon_n,\ \boldsymbol x_n\in\R^d,\ t\in\R</script>
where $\varepsilon_n$ is a continuous rv such that $\mathbb E[\varepsilon_n]=0$ and $\text{Var}(\varepsilon_n)=\sigma^2$. Let‚Äôs assume again that we further model $\varepsilon_n\sim\mathcal N\left(0,\sigma^2\right)$, and:
<script type="math/tex">f(\boldsymbol x)\approx y(\boldsymbol x;\boldsymbol\beta)=\sum_{i=0}^d\beta_ix_i=\boldsymbol\beta^T\boldsymbol x</script>
with $\boldsymbol x=(1,x_1,\ldots,x_d)^T$ and $\boldsymbol\beta=(\beta_0,\beta_1,\ldots,\beta_d)^T$. Suppose we have an iid sample of $N$ labeled observations $D={(\boldsymbol x_n,t_n)}<em>{n=1,\ldots,N}$, where $\boldsymbol x_n\in\mathbb{R}^d,t_n\in\mathbb{R}$. Therefore, our statistical model is $t_n\sim\mathcal N\left(y(\boldsymbol x_n;\boldsymbol\beta),\sigma^2\right)$ or:
<script type="math/tex">p(t_n|\boldsymbol x_n;\theta)=\frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(-\frac{1}{2\sigma^2}\left(t_n-\boldsymbol\beta^T\boldsymbol x_n\right)^2\right),</script>
with unknown parameters $\theta:={\beta_0,\beta_1,\ldots,\beta_p,\sigma^2}$. Put $\boldsymbol t=(t_1,\ldots,t_N)^T$ and $X</em>{N\times(d+1)}$ the matrix of the $\boldsymbol x_n$. Define the <strong>likelihood</strong> as $\mathcal L(\theta):=P(\boldsymbol t\vert X;\theta)$. Let us maximize the log-likelihood:
<script type="math/tex">l(\theta):=\log\mathcal L(\theta)=\log\prod_{n=1}^Np(t_n|\boldsymbol x_n;\theta)=\sum_{n=1}^N\log{p(t_n|\boldsymbol x_n;\theta)}=\\
=-\frac{N}{2}\log\left(2\pi\sigma^2\right)-\frac{1}{2\sigma^2}\sum_{n=1}^N\left(t_n-\boldsymbol\beta^T\boldsymbol x_n\right)^2=\\
=-\frac{N}{2}\log\left(2\pi\sigma^2\right)-\frac{1}{2\sigma^2}(\boldsymbol t-X\boldsymbol\beta)^T(\boldsymbol t-X\boldsymbol\beta)=\\
=-\frac{N}{2}\log\left(2\pi\sigma^2\right)-\frac{1}{2\sigma^2}\|\boldsymbol t-X\boldsymbol\beta\|^2.</script>
If we derive this wrt $\boldsymbol\beta$ and $\sigma^2$, and set equal to zero, we get:
<script type="math/tex">\frac{\part l}{\part\boldsymbol\beta}=-\frac{1}{2\sigma^2}\left(-2X^T\boldsymbol t+2X^TX\boldsymbol\beta\right)=0\\
\frac{\part l}{\part\sigma^2}=-\frac{N}{2\sigma^2}+\frac{1}{2\sigma^4}(\boldsymbol t-X\boldsymbol\beta)^T(\boldsymbol t-X\boldsymbol\beta)=0.\\</script>
Therefore, we can calculate the estimates for both parameters:
<script type="math/tex">\boldsymbol{\hat\beta}=\left(X^TX\right)^{-1}X^T\boldsymbol t,\\
\hat\sigma^2=\frac{1}{N}(\boldsymbol t-X\boldsymbol{\hat\beta})^T(\boldsymbol t-X\boldsymbol{\hat\beta})=\frac{1}{N}\|\boldsymbol t-X\boldsymbol{\hat\beta}\|^2.</script>
Note that $\hat\sigma^2=R_{\text{emp}}(y_D)$, which is a <strong>biased estimator</strong> for $\sigma^2$. An unbiased estimator is
<script type="math/tex">\bar\sigma^2=\frac{N}{N-d}\hat\sigma^2.</script>
It‚Äôs also known that $\boldsymbol{\hat\beta}$ is an unbiased estimator of $\boldsymbol\beta$ and that $\text{Var}(\boldsymbol{\hat\beta})=\left(X^TX\right)^{-1}\sigma^2$. All of this implies that $\boldsymbol{\hat\beta}\sim\mathcal N\left(\boldsymbol\beta,\left(X^TX\right)^{-1}\sigma^2\right)$.</p>

<p>The matrix $X^+=\left(X^TX\right)^{-1}X^T$ is known as the <strong>Moore-Penrose pseudo-inverse</strong> of $X$. It is the generalization of the notion of an inverse matrix to non-square matrices. It has the property that $X^+X=I$, although in general $XX^+\neq I$. However, both are symmetric.</p>

<p><strong>Theorem.</strong> Let $X_{N\times M}$, with $N&gt;M$. If the column vectors of $X$ are linearly independent, <em>i.e.</em>, if $\text{rank}(X)=M$, then:</p>

<ol>
  <li>The matrix $X^TX$ is symmetric and positive definite. In particular, it is non-singular.</li>
  <li>The least squares problem</li>
</ol>

<script type="math/tex; mode=display">\min_{\boldsymbol\beta\in\R^M}\|\boldsymbol t-X\boldsymbol\beta\|^2,</script>

<p>‚Äã		has a unique solution.</p>

<ol>
  <li>This solution can be found solving the so-called Gauss‚Äô normal equations,</li>
</ol>

<script type="math/tex; mode=display">\left(X^TX\right)\boldsymbol\beta=X^T\boldsymbol t</script>

<p>‚Äã		for $\boldsymbol\beta$.</p>

<h3 id="quality-of-the-fit">Quality of the fit</h3>

<ul>
  <li>In statistics, $-2l=-2\log\mathcal L$ is called the <strong>deviance</strong>.</li>
  <li>In ML, this quality measure is the <strong>square error</strong>:</li>
</ul>

<script type="math/tex; mode=display">N\log\left(2\pi\sigma^2\right)+\frac{1}{\sigma^2}\|\boldsymbol t-X\boldsymbol{\hat\beta}\|^2</script>

<ul>
  <li>A much better quantity to report is the $\text{NRMSE}$,</li>
</ul>

<script type="math/tex; mode=display">\text{NRMSE}(\boldsymbol{\hat\beta})=\sqrt{\frac{\|\boldsymbol t-X\boldsymbol{\hat\beta}\|^2}{(N-1)\text{Var}(\boldsymbol t)}}.</script>

<p>In statistics, $R^2=1-\text{NRMSE}^2$ is the proportion of the target variability <em>explained</em> by the model.</p>

<h3 id="leaping-forward-basis-functions">Leaping forward: basis functions</h3>

<p>Recall that a model is <strong>linear</strong> if up to an invertible function its parameters play a linear role in the model. For example,
<script type="math/tex">y(x;\boldsymbol\beta)=\sum_{j=0}^d\beta_jx^j,\ x\in\mathbb{R}</script>
is a polynomial on $x$, but also a linear model on $\boldsymbol\beta$.</p>

<p>A simple but powerful idea is the introduction of <strong>basis functions</strong>:
<script type="math/tex">y(\boldsymbol x;\boldsymbol w)=\sum_{j=0}^Mw_j\varphi_j(\boldsymbol x)=\boldsymbol w^T\boldsymbol\varphi(\boldsymbol x),</script>
where $\varphi_0(\boldsymbol x)=1,\boldsymbol\varphi(\boldsymbol x)=\left(1,\varphi_1(\boldsymbol x),\ldots,\varphi_M(\boldsymbol x)\right)^T,\boldsymbol w=\left(w_0,w_1,\ldots,w_M\right)^T$. This is still a <strong>linear model</strong>. Define $\boldsymbol t=\left(t_1,\ldots,t_N\right)^T$ as the vector of targets, and $\boldsymbol\varphi_{N\times(M+1)}$ as the matrix of the $\boldsymbol\varphi_{ij}=\varphi_j(\boldsymbol x_i),i=1,\ldots,N,j=1,\ldots,M$:
<script type="math/tex">% <![CDATA[
\boldsymbol\varphi=\begin{pmatrix}
1 & \varphi_1(\boldsymbol x_1) & \varphi_2(\boldsymbol x_1) & \cdots & \varphi_M(\boldsymbol x_1) \\
1 & \varphi_1(\boldsymbol x_2) & \varphi_2(\boldsymbol x_2) & \cdots & \varphi_M(\boldsymbol x_2) \\
\cdots & \cdots & \cdots & \ddots & \cdots \\
1 & \varphi_1(\boldsymbol x_N) & \varphi_2(\boldsymbol x_N) & \cdots & \varphi_M(\boldsymbol x_N) \\
\end{pmatrix}. %]]></script>
So, let us maximize the new log-likelihood: the Gauss‚Äô normal equations are
<script type="math/tex">\left(\boldsymbol\varphi^T\boldsymbol\varphi\right)w=\boldsymbol\varphi^T\boldsymbol t</script>
and their solution is
<script type="math/tex">\boldsymbol{\hat w}=\left(\boldsymbol\varphi^T\boldsymbol\varphi\right)^{-1}\boldsymbol\varphi^T\boldsymbol t=\boldsymbol\varphi^+\boldsymbol t,\\
\hat\sigma^2=\frac{1}{N}(\boldsymbol t-\boldsymbol\varphi\boldsymbol{\hat w})^T(\boldsymbol t-\boldsymbol\varphi\boldsymbol{\hat w})=\frac{1}{N}\|\boldsymbol t-\boldsymbol\varphi\boldsymbol{\hat w}\|^2.</script></p>

<h3 id="singular-value-decomposition">Singular Value Decomposition</h3>

<p>The direct computation of the pseudo-inverse of $\boldsymbol\varphi$ has two major drawbacks:</p>

<ul>
  <li>When $M$ is large, $\boldsymbol\varphi^T\boldsymbol\varphi$ is a large $(M+1)\times(M+1)$ matrix; then, the computation of the required inverse $\left(\boldsymbol\varphi^T\boldsymbol\varphi\right)^{-1}$ can be costly.</li>
  <li>If $\boldsymbol\varphi^T\boldsymbol\varphi$ is singular, or close to, then the required inverse $\left(\boldsymbol\varphi^T\boldsymbol\varphi\right)^{-1}$ can be impossible, or numerically delicate.</li>
</ul>

<p><strong>Theorem.</strong> Every matrix $X_{N\times M}$ can be expressed as $X=U\Delta V^T$, with $U\in\mathcal M_N(\mathbb{R})$,$V\in\mathcal M_M(\mathbb{R})$,$\Delta\in\mathcal M_{N\times M}(\mathbb{R})$ diagonal. The columns of $U$ are the eigenvectors of $XX^T$, and the columns of $V$ are the eigenvectors of $X^TX$.</p>

<p>Let $\text{rank}(X)=r\leq\min(N,M)$. Then exactly $r$ elements $\lambda_k$ in the diagonal of $\Delta$ are strictly positive; the remaining elements are null. These $\lambda_k&gt;0$ are called the <strong>singular values</strong> and correspond to the square roots of the positive eigenvalues of $XX^T$ (same as $X^TX$).</p>

<p>Sometimes an <em>economy</em> size decomposition is delivered: If $X$ is $N\times M$ with $N&gt;M$, then only the first $M$ columns of $U$ are given and $\Delta$ is $M\times M$.</p>

<h4 id="svd-for-least-squares">SVD for least squares</h4>

<p>Given the least squares problem
<script type="math/tex">\min_{\boldsymbol w\in\R^M}\|\boldsymbol t-X\boldsymbol w\|^2,</script>
the solution can be obtained with the SVD as:</p>

<ul>
  <li>Compute the economy size SVD of $X=U\Delta V^T$.</li>
  <li>Solve for $\boldsymbol w$ as $\boldsymbol{\hat w}=V\text{diag}\left(\lambda_k^{-1}\right)U^T\boldsymbol t$, where only the $\lambda_k&gt;0$ are considered.</li>
</ul>

<h3 id="regularized-least-squares">Regularized least squares</h3>

<p>The maximum likelihood framework can yield unstable parameter estimates, specially when</p>

<ul>
  <li>the explanatory variables are highly correlated;</li>
  <li>there is an insufficient number of observations $(N)$ relative to the number of predictors (basis functions $M+1$ or dimensions $d+1$).</li>
</ul>

<p>In the context of regression with Gaussian noise (square error), it is quite common to penalize the parameter vector. Define the <strong>penalized empirical error</strong> as:
<script type="math/tex">R_{\text{emp}}(y(\cdot;\boldsymbol w)):=\|\boldsymbol t-\boldsymbol\varphi\boldsymbol w\|^2+\lambda\|\boldsymbol w\|^2,\ \lambda>0.</script>
If we set its derivative wrt $\boldsymbol w$ equal to zero
<script type="math/tex">-2\boldsymbol\varphi^T\boldsymbol t+2\boldsymbol\varphi^T\boldsymbol\varphi\boldsymbol w+2\lambda\boldsymbol w=0,</script>
we solve for $\boldsymbol w$ and we get
<script type="math/tex">\boxed{
\boldsymbol{\hat w}=(\boldsymbol\varphi^T\boldsymbol\varphi+\lambda I)^{-1}\boldsymbol\varphi^T\boldsymbol t.
}</script>
This is known as <strong>Tikhonov</strong> or $L_2$ <strong>regularization</strong> in ML. Perhaps it‚Äôs best known as <strong>ridge regression</strong> in statistics, where it‚Äôs usually explained as a ‚Äúpenalized log-likelihood‚Äù. This can also be derived from Bayesian statistics arguments. Tikhonov regularization has some advantages:</p>

<ul>
  <li>Pushing the length of the parameter vector $\vert \boldsymbol w\vert $ to $0$ allows the fit to be under explicit control with the regularization parameter $\lambda$.</li>
  <li>The matrix $\boldsymbol\varphi^t\boldsymbol\varphi$ is positive semi-definite; therefore $\boldsymbol\varphi^T\boldsymbol\varphi+\lambda I$ is guaranteed to be positive definite (hence non-singular), for all $\lambda&gt;0$.</li>
</ul>
:ET