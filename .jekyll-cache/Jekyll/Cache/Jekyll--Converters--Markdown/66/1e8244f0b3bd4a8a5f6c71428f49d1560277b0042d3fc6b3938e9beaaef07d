I"–Q<h1 id="an√†lisi-de-dades">An√†lisi de Dades</h1>

<p>[TOC]</p>

<h2 id="1-introducci√≥-i-pre-processament-de-dades">1. Introducci√≥ i pre-processament de dades</h2>

<h3 id="11-introducci√≥">1.1. Introducci√≥</h3>

<p>Estudiarem dos tipus de dades en aquest curs:</p>

<ul>
  <li>Conjunts de dades multivariades (moltes observacions i moltes variables).</li>
  <li>S√®ries temporals (variables que s‚Äôobserven de manera repetida en el temps, di√†riament, setmanalment, mensualment, etc).</li>
</ul>

<p>Les sub-disciplines de l‚Äôestad√≠stica dedicades a l‚Äôan√†lisi de dades aix√≠ s√≥n <strong>l‚Äôan√†lisi multivariat</strong> i <strong>l‚Äôan√†lisi de s√®ries temporals</strong>, i tractarem ambdues t√®cniques en aquest curs.</p>

<h3 id="12-pre-processament">1.2. Pre-processament</h3>

<p>Generalment no es recomana ajustar models estad√≠stics directament sobre <em>datasets</em> sense processar. Existeixen moltes t√®cniques de <span style="color:blue">netejat de dades</span> i <span style="color:blue">pre-processament</span> que ajuden a assegurar la qualitat de les dades i que faciliten una an√†lisi amb m√©s sentit. El pre-processament presenta diverses caracter√≠stiques importants:</p>

<ul>
  <li>Conversi√≥ de l‚Äôarxiu de dades a un format adequat.</li>
  <li>Evitar duplicacions.</li>
  <li>Comprovar l‚Äôexist√®ncia de <span style="color:blue">dades mancants</span>, <em><span style="color:blue">outliers</span></em> i <span style="color:blue">zeros</span>.</li>
  <li>Comprovar si s‚Äôhan de fer transformacions sobre les dades.</li>
</ul>

<h3 id="13-valors-mancants">1.3. Valors mancants</h3>

<p><strong>Treballant amb valors mancants:</strong> N‚Äôexisteixen? Com estan codificats? Quin percentatge de les dades falta? Es concentren els valors mancants en algunes variables o individus?</p>

<h4 id="classificaci√≥-dels-valors-mancants">Classificaci√≥ dels valors mancants</h4>

<ol>
  <li><em>Missing Completely At Random</em> (MCAR):
    <ul>
      <li>Hi ha valors mancants, per√≤ es pot intentar veure un <em>dataset</em> hipot√®tic d‚Äôindividus observats completament.</li>
      <li>Si les observacions s√≥n una mostra aleat√≤ria d‚Äôaquestes dades ideals, aleshores les dades mancants s√≥n MCAR.</li>
      <li>Les observacions que falten s√≥n tamb√© observacions aleat√≤ries d‚Äôaquest <em>dataset</em>.</li>
      <li>Descartar els valors mancants no √©s molt problem√†tic si no n‚Äôhi ha gaires.</li>
    </ul>
  </li>
  <li><em>Missing At Random</em> (MAR):
    <ul>
      <li>La probabilitat que un resultat falti per una variable particular pot dependre de les altres dades observades (per exemple d‚Äôaltres variables que s√≠ que han estat observades).</li>
      <li>Condicionat a les dades observades, aquesta probabilitat pot no dependre dels valors de la variable.</li>
    </ul>
  </li>
  <li><em>Missing Not At Random</em> (MNAR):
    <ul>
      <li>La probabilitat d‚Äôun valor mancant dep√®n en els valors de la variable en consideraci√≥, incl√∫s despr√©s de controlar les relacions amb altres variables rellevants.</li>
    </ul>
  </li>
</ol>

<h4 id="opcions-dactuaci√≥">Opcions d‚Äôactuaci√≥</h4>

<h5 id="esborrar-dades-mancants">Esborrar dades mancants</h5>

<p>Si esborrem les dades mancants, la mida de la mostra es redueix, i perdem poder per detectar efectes interessants. La infer√®ncia estad√≠stica podria estar esbiaixada si les dades mancants no s√≥n MCAR.</p>

<h5 id="imputaci√≥-de-dades-mancants">Imputaci√≥ de dades mancants</h5>

<h6 id="imputaci√≥-per-la-mitjana">Imputaci√≥ per la mitjana</h6>

<p>Se substitueixen els valors mancants per la mitjana de les observacions. Aix√≤ atenua l‚Äôefecte de la variable explicativa sobre la variable resposta.</p>

<p><img src="C:\Users\alexb\AppData\Roaming\Typora\typora-user-images\image-20200213093808158.png" alt="image-20200213093808158" /></p>

<h6 id="imputaci√≥-per-regressi√≥">Imputaci√≥ per regressi√≥</h6>

<p>Se substitueixen els valors mancants per el valor de la recta de regressi√≥. Es podria subestimar la vari√†ncia real de les dades.</p>

<p><img src="C:\Users\alexb\AppData\Roaming\Typora\typora-user-images\image-20200213093833105.png" alt="image-20200213093833105" /></p>

<h6 id="imputaci√≥-per-regressi√≥-estoc√†stica">Imputaci√≥ per regressi√≥ estoc√†stica</h6>

<p>Es substitueixen els punts pels valors de la regressi√≥ lineal m√©s un soroll tret de $\mathcal{N}(0,s_e^2)$.</p>

<p><img src="C:\Users\alexb\AppData\Roaming\Typora\typora-user-images\image-20200213093846857.png" alt="image-20200213093846857" /></p>

<h6 id="imputaci√≥-m√∫ltiple">Imputaci√≥ m√∫ltiple</h6>

<p>S‚Äôaplica molts cops la imputaci√≥ per regressi√≥ estoc√†stica i es fa cada cop la recta de nou. Finalment, el valor se substitueix per la mitjana de tots aquests.</p>

<p><img src="C:\Users\alexb\AppData\Roaming\Typora\typora-user-images\image-20200213093901096.png" alt="image-20200213093901096" /></p>

<h3 id="14-zeros">1.4. Zeros</h3>

<p>Tractar zeros √©s una operaci√≥ delicada, ja que es poden confondre els zeros per valors mancants (per exemple, com fa MS Excel amb les cel¬∑les en blanc). Per tant, √©s molt important la manera de codificar les dades, i resulta essencial utilitzar un <strong>codi especial a la base de dades</strong> per indicar si la dada √©s, en efecte, mancant. Per exemple, en <code class="highlighter-rouge">R</code>, fem servir ` NA` pels valors mancants.</p>

<h3 id="15-outliers">1.5. <em>Outliers</em></h3>

<p>Podem tenir <em>outliers</em> univariats, bivariats, o multivariats. Els dos primers els podem identificar amb diagrames univariants i bivariants, per√≤ per detectar els multivariats √©s m√©s complicat.</p>

<p><strong>Com s‚Äôactua sobre els <em>outliers</em>?</strong></p>

<ul>
  <li>Primer cal comprovar si el valor correspon a una mesura correcta o √©s clarament err√≤nia o impossible. Per exemple, es pot consultar a la gent que ha generat les dades en q√ºesti√≥.</li>
  <li>Depenent de la t√®cnica estad√≠stica utilitzada, un <em>outlier</em> pot ser problem√†tic o no. Si es considera problem√†tica l‚Äôexist√®ncia d‚Äô<em>outliers</em>, es pot considerar una transformaci√≥ de les dades per atenuar-ne l‚Äôefecte.</li>
</ul>

<p>Per tant, es fa l‚Äôan√†lisi amb i sense els <em>outliers</em>, i es comparen resultats.</p>

<h3 id="16-transformacions">1.6. Transformacions</h3>

<p>En estad√≠stica s‚Äôutilitzen molt les transformacions de les variables aleat√≤ries. Per quins motius s‚Äôutilitzen?</p>

<ul>
  <li>Per reduir els efectes dels <em>outliers</em>.</li>
  <li>Per fer una distribuci√≥ m√©s sim√®trica.</li>
  <li>Per produir homoscedasticitat.</li>
  <li>Per aconseguir normalitat aproximada.</li>
  <li>Per eliminar una restricci√≥ que opera sobre les dades</li>
  <li>‚Ä¶</li>
</ul>

<p><strong><span style="color:green">Transformacions comunes</span>:</strong></p>

<ul>
  <li>$y=\ln(x)$ (zeros no permesos).</li>
  <li>$y=\sqrt{x}$ (zeros permesos).</li>
  <li>Transformaci√≥ <em>logit</em> per probabilitats $y=\ln\left(\frac{p}{1-p}\right).$</li>
  <li>Substituir les observacions pel seu rang (?) /// la seva posici√≥ (?).</li>
  <li>Transformaci√≥ de la pot√®ncia $y=x^a$.</li>
  <li>Transformaci√≥ de Box-Cox.</li>
</ul>

<h4 id="transformaci√≥-de-box-cox">Transformaci√≥ de Box-Cox</h4>

<p>La transformaci√≥ de Box-Cox es pot utilitzar per aconseguir normalitat aproximada en una variable aleat√≤ria. La seva expressi√≥ anal√≠tica √©s
<script type="math/tex">y_i^{(\lambda)}=\begin{cases}\frac{y_i^\lambda-1}{\lambda}\ \ \quad\lambda\neq0\\ \ln(y_i)\quad\lambda=0\end{cases}.</script>
El valor √≤ptim de $\lambda$ s‚Äôaconsegueix per m√†xima versemblan√ßa.</p>

<h2 id="2-√†lgebra-de-matrius-aplicada-a-lan√†lisi-multivariant">2. √Älgebra de matrius aplicada a l‚Äôan√†lisi multivariant</h2>

<h3 id="resum-de-vectors-i-matrius">Resum de vectors i matrius</h3>

<ul>
  <li>
    <table>
      <tbody>
        <tr>
          <td><strong>Norma-2:</strong> $|x|<em>2=\sqrt{x^Tx}=\sqrt{\sum</em>{i=1}^dx_i^2};|\alpha x|_2=</td>
          <td>\alpha</td>
          <td>\cdot|x|_2.$</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li><strong>Producte escalar:</strong> $&lt;x,y&gt;_2=x^Ty=x_1y_1+\cdots+x_dy_d;&lt;x,y&gt;=0\iff x\perp y.$</li>
  <li><strong>Angle:</strong> $\cos{\theta}=\frac{&lt;x,y&gt;_2}{|x|_2\cdot|y|_2}.$</li>
  <li><strong>Descomposici√≥ espectral i SVD:</strong></li>
</ul>

<h3 id="matrius-b√†siques-a-lan√†lisi-multivariant">Matrius b√†siques a l‚Äôan√†lisi multivariant</h3>

<ul>
  <li><strong>Matriu de dades:</strong> $X_{n\times p}.$</li>
  <li><strong>Vector de mitjana mostral:</strong> $\boldsymbol m_{p\times1}=\left(\frac{1}{d}\boldsymbol 1^TX\right)^T.$</li>
  <li><strong>Matriu de dades centrades:</strong> $X_c=X-\boldsymbol 1_{n\times1}\boldsymbol m^T=\left(I-\frac{1}{n}\boldsymbol1\boldsymbol1^T\right)X.$</li>
  <li><strong>Matriu centradora:</strong> $H=I-\frac{1}{n}\boldsymbol1\boldsymbol1^T\implies X_c=HX.$</li>
  <li><strong>Matriu de dades estandaritzades:</strong> $X_s=X_cD^{-1}$, on $D_s=\text{diag}(s_1,\ldots,s_p)$ √©s la matriu de les desviacions t√≠piques de les columnes de $X$. Per tant, $X_s=HXD_s^{-1}.$</li>
  <li><strong>Matriu de covari√†ncies mostrals:</strong> $S=\frac{1}{n-1}X_c^TX_c.$</li>
  <li><strong>Matriu de correlacions mostrals:</strong> $R=D_s^{-1}SD_s^{-1}=\frac{1}{n-1}D_s^{-1}X^TH^THXD_s^{-1}=\frac{1}{n-1}X_s^TX_s.$</li>
</ul>

<h4 id="dist√†ncies-importants-a-lan√†lisi-multivariant">Dist√†ncies importants a l‚Äôan√†lisi multivariant</h4>

<ul>
  <li><strong>Euclidiana:</strong></li>
</ul>

<script type="math/tex; mode=display">\delta_{rs}=\sqrt{(x_s-x_s)^T(x_r-x_s)}=\left(\sum_{i=1}^p|x_{ri}-x_{si}|^2\right)^\frac{1}{2}.</script>

<ul>
  <li><strong>Dist√†ncia de Mahalanobis:</strong></li>
</ul>

<script type="math/tex; mode=display">\delta_{rs}=\left((x_r-x_s)^TS^{-1}(x_r-x_s)\right)^\frac{1}{2}.</script>

<ul>
  <li><strong>Dist√†ncia de Minkowski:</strong></li>
</ul>

<script type="math/tex; mode=display">\delta_{rs}=\left(\sum_{i=1}^p|x_{ri}-x_{si}|^\lambda\right)^\frac{1}{\lambda}.</script>

<h2 id="3-an√†lisi-de-components-principals-pca">3. An√†lisi de components principals (PCA)</h2>

<p>Els objectius principals de l‚Äô<strong>An√†lisi de components principals (PCA)</strong> s√≥n:</p>

<ul>
  <li>Reduir el nombre de variables.</li>
  <li>Una visualitzaci√≥ de la matriu de dades a trav√©s d‚Äôun <em>biplot</em>.</li>
</ul>

<h3 id="teoria-de-la-pca">Teoria de la PCA</h3>

<p>Busquem combinacions lineals de les variables originals,
<script type="math/tex">F_1=a_{11}X_1+a_{12}X_2+\cdots+a_{1p}X_p\\
F_2=a_{21}X_1+a_{22}X_2+\cdots+a_{2p}X_p\\
\vdots\\
F_p=a_{p1}X_1+a_{p2}X_2+\cdots+a_{pp}X_p\\</script>
tals que satisfacin:</p>

<ul>
  <li>$F_1,F_2,\ldots,F_p$ no correlades.</li>
  <li>$\text{Var}(F_1)$ m√†xima.</li>
  <li>$\text{Var}(F_1)\geq\text{Var}(F_2)\geq\cdots\geq\text{Var}(F_p)$.</li>
  <li>$a_{i1}^2+a_{i2}^2+\cdots+a_{ip}^2=1.\qquad(-1\leq a_{ij}\leq1)$</li>
</ul>

<p>Tots els coeficients i valors propis es poden obtenir de la <strong>descomposici√≥ espectral</strong> de la matriu de covari√†ncies, $S=AD_\lambda A^T$. Les <strong>components principals</strong> s‚Äôobtenen calculant
<script type="math/tex">% <![CDATA[
\begin{matrix}
F_p & = & X_c & A.\\
(n\times p) & & (n\times p) & (p\times p)
\end{matrix} %]]></script>
Els valors propis corresponen a les vari√†ncies de les components principals perqu√®
<script type="math/tex">\frac{1}{n-1}F_p^TF_p=\frac{1}{n-1}(X_cA)^TX_cA=\frac{1}{n-1}A^TX_c^TX_cA=A^TSA=A^TAD_\lambda A^TA=D_\lambda.</script>
Una manera alternativa de fer PCA √©s utilitzant la <strong>descomposici√≥ en valors singulars (SVD)</strong> de la matriu de dades centrada, $X_c=UDA^T$. Les components principals aleshores es calculen amb $F_p=X_cA=UD$. Els valors singulars al quadrat es relacionen amb la vari√†ncia dels components principals perqu√®
<script type="math/tex">\frac{1}{n-1}F_p^TF_p=\frac{1}{n-1}(UD)^TUD=\frac{1}{n-1}D^2=D_\lambda.</script>
Aquesta aproximaci√≥ √©s molt convenient per construir <em>biplots</em>.</p>

<h3 id="biplots"><em>Biplots</em></h3>

<p>Un <em>biplot</em> √©s una eina molt √∫til per explorar gr√†ficament dades multivariants. √âs una generalitzaci√≥ multivariant de l‚Äô<em>scatterplot</em>, per√≤ difereix d‚Äôaquest en alguns aspectes:</p>

<ul>
  <li>T√≠picament t√© m√©s de dos eixos.</li>
  <li>Els eixos no s√≥n perpendiculars, sin√≥ que tendeixen a ser oblics.</li>
  <li>La matriu de dades es representa de manera aproximada, no exacta.</li>
</ul>

<p>Un <em>biplot</em> √©s una visualitzaci√≥ de les files i columnes d‚Äôuna matriu que √©s √≤ptima en termes de m√≠nims quadrats.</p>

<p>Per fer un <em>biplot</em> d‚Äôuna matriu, primer l‚Äôhem de factoritzar
<script type="math/tex">X_{n\times p}=F_{n\times r}G_{r\times p}^T\qquad(1)</script>
en el producte d‚Äôuna matriu de <strong>marcadors de files</strong>, $F$, i una matriu de <strong>marcadors de columnes</strong>, $G$. Aquesta factoritzaci√≥ tamb√© existeix en un scatterplot ordinari,
<script type="math/tex">X_{n\times2}=X_{n\times2}I_{2\times2}.</script>
La factoritzaci√≥ $(1)$ no √©s √∫nica; amb qualsevol aplicaci√≥ lineal invertible $T$ tenim
<script type="math/tex">X_{n\times p}=F_{n\times r}TT^{-1}G_{r\times p}^T=\tilde F_{n\times r}\tilde G_{r\times p}^T.\qquad(2)</script></p>

<h4 id="biplots-i-el-producte-escalar"><em>Biplots</em> i el producte escalar</h4>

<p>En un <em>biplot</em>, els valors de les dades s‚Äôaproximen amb el <strong>producte escalar ordinari</strong> entre dos vectors:</p>

<p><img src="AD.assets/image-20200227095710124.png" alt="image-20200227095710124" style="zoom:80%;" /></p>

<p>Veiem que, efectivament,
<script type="math/tex">\cos\theta=\frac{\|p_i\|}{\|f_i\|}=\frac{f_i^Tg_j}{\|f_i\|\cdot\|g_i\|},\quad \|p_i\|=\frac{f_i^Tg_j}{\|g_j\|},\quad x_{ij}\approx f_i^Tg_j=\|p_i\|\cdot\|g_j\|.</script></p>

<h4 id="biplots-a-la-pca"><em>Biplots</em> a la PCA</h4>

<p>La PCA d√≥na un <em>biplot</em> de la matriu de dades centrades. S‚Äôobt√© representant conjuntament les dues primeres components principals (les primeres dues columnes de $F_p$) i els dos primers vectors propis (primeres dues columnes de $G_s$). Les files de $F_p$ es representen normalment amb punts, i les files de $G_s$ amb fletxes. Les coordenades $F_p$ s‚Äôanomenen <span style="color:blue">coordenades principals</span>, i les coordenades $G_s$ s‚Äôanomenen <span style="color:blue">coordenades est√†ndar</span>. Aquestes √∫ltimes compleixen $G_s^TG_s=I$.</p>

<p>Es pot fer un escalat alternatiu del <em>biplot</em>. Utilitzant $D_s$ la matriu diagonal amb les desviacions t√≠piques de les components principals,
<script type="math/tex">X_c=F_pD_s^{-1}D_sG_s^T=F_sD_sG_s^T=F_sG_p^T,\\ G_p=G_sD_s.</script>
Aquest <em>biplot</em> representa les <span style="color:blue">components principals estandaritzades</span>, $F_s=F_pD_s^{-1}$. Tenim, per tant, dos <em>biplots</em>:</p>

<ul>
  <li>$X_c=F_pG_s^T$ (<em>biplot</em> de <span style="color:blue">forma</span>)</li>
  <li>$X_c=F_sG_p^T$ (<em>biplot</em> de <span style="color:blue">covari√†ncia</span>)</li>
</ul>

<p>En general, els <em>biplots</em> de forma es centren en la <strong>representaci√≥ de dist√†ncies</strong>, mentre que els <em>biplots</em> de covari√†ncia es centren en representar la <strong>estructura de correlaci√≥</strong>.</p>

<h4 id="propietats-dels-biplots">Propietats dels <em>biplots</em></h4>

<ul>
  <li>En el <em>biplot</em> de forma, les dist√†ncies euclidianes entre punts aproximen les dist√†ncies euclidianes entre files de la matriu de dades.</li>
  <li>En el <em>biplot</em> de covari√†ncia:
    <ul>
      <li>Les dist√†ncies euclidianes entre punts aproximen les dist√†ncies de Mahalanobis entre files de la matriu de dades.</li>
      <li>La longitud d‚Äôuna fletxa aproxima la desviaci√≥ t√≠pica de la variable corresponent.</li>
      <li>L‚Äôangle entre dues fletxes aproxima la correlaci√≥ entre les variables corresponents.</li>
    </ul>
  </li>
</ul>

<h4 id="biplots-a-r"><em>Biplots</em> a R</h4>

<p>Amb aquestes instruccions</p>

<div class="language-R highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plot</span><span class="p">(</span><span class="nb">F</span><span class="p">[,</span><span class="w"> </span><span class="m">1</span><span class="p">],</span><span class="w"> </span><span class="nb">F</span><span class="p">[,</span><span class="w"> </span><span class="m">2</span><span class="p">],</span><span class="w"> </span><span class="n">pch</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">19</span><span class="p">)</span><span class="w">
</span><span class="n">point</span><span class="p">(</span><span class="n">G</span><span class="p">[,</span><span class="w"> </span><span class="m">1</span><span class="p">],</span><span class="w"> </span><span class="n">G</span><span class="p">[,</span><span class="w"> </span><span class="m">2</span><span class="p">],</span><span class="w"> </span><span class="n">pch</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">2</span><span class="p">,</span><span class="w"> </span><span class="n">col</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"blue"</span><span class="p">)</span><span class="w">
</span><span class="n">arrows</span><span class="p">(</span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="n">G</span><span class="p">[,</span><span class="w"> </span><span class="m">1</span><span class="p">],</span><span class="w"> </span><span class="n">G</span><span class="p">[,</span><span class="w"> </span><span class="m">2</span><span class="p">])</span><span class="w">
</span></code></pre></div></div>

<p>podem representar el seg√ºent <em>biplot</em>:</p>

<p><img src="AD.assets/image-20200227093108970.png" alt="image-20200227093108970" style="zoom:80%;" /></p>

<h3 id="quantes-components-utilitzem">Quantes components utilitzem?</h3>

<p>Els criteris a considerar s√≥n els seg√ºents:</p>

<ul>
  <li>El <strong>percentatge de vari√†ncia explicada</strong>; volem que sigui $&gt;80\%$.</li>
  <li>El <strong>m√≤dul del valor propi corresponent</strong>; volem valors propis $&gt;\bar\lambda$.</li>
  <li>L‚Äô<strong>scree-plot</strong>; ens dona una visualitzaci√≥ gr√†fica dels valors propis, √©s a dir, de quanta vari√†ncia explica cada component.</li>
  <li>Tests de signific√†ncia amb els valors propis.</li>
</ul>

<p>Si ho plantegem matem√†ticament,
<script type="math/tex">\text{tr}(S)=\text{tr}(AD_\lambda A^T)=\text{tr}(D_\lambda),\\
\sum_{i=1}^p\text{Var}(X_i)=\sum_{i=1}^p\text{Var}(F_i)=\sum_{i=1}^p\lambda_i,</script></p>

<table>
  <thead>
    <tr>
      <th>Component</th>
      <th>$F_1$</th>
      <th>$F_2$</th>
      <th>$\cdots$</th>
      <th>$F_p$</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Vari√†ncia</strong></td>
      <td>$\lambda_1$</td>
      <td>$\lambda_2$</td>
      <td>$\cdots$</td>
      <td>$\lambda_p$</td>
    </tr>
    <tr>
      <td><strong>Fracci√≥</strong></td>
      <td>$\lambda_1/\sum\lambda_i$</td>
      <td>$\lambda_2/\sum\lambda_i$</td>
      <td>$\cdots$</td>
      <td>$\lambda_p/\sum\lambda_i$</td>
    </tr>
    <tr>
      <td><strong>Fracci√≥ acumulada</strong></td>
      <td>$\lambda_1/\sum\lambda_i$</td>
      <td>$(\lambda_1+\lambda_2)/\sum\lambda_i$</td>
      <td>$\cdots$</td>
      <td>$\sum\lambda_i/\sum\lambda_i$</td>
    </tr>
  </tbody>
</table>

<h4 id="tipus-de-pca">Tipus de PCA</h4>

<p>Hi ha dos tipus de PCA. Els c√†lculs es poden basar en</p>

<ul>
  <li>La <strong>matriu de covari√†ncies</strong> $S$:
    <ul>
      <li>No √©s invariant respecte l‚Äôescala de mesura.</li>
      <li>La variable amb vari√†ncia m√©s gran domina.</li>
    </ul>
  </li>
  <li>La <strong>matriu de correlacions</strong> $R$:
    <ul>
      <li>√âs invariant respecte l‚Äôescala de mesura.</li>
      <li>Totes les variables contribueixen de la mateixa manera.</li>
    </ul>
  </li>
</ul>

<h4 id="interpretaci√≥">Interpretaci√≥</h4>

<p>Les components es poden interpretar amb l‚Äôajuda dels coeficients de les variables, de les correlacions entre variables i components, i amb el <em>biplot</em>. Si l‚Äôobjectiu √©s tenir una imatge de la matriu de dades, aleshores la interpretaci√≥ de les components podria no ser necess√†ria.</p>

<h3 id="sobre-la-bondat-dajust">Sobre la bondat d‚Äôajust</h3>

<p>Dels valors propis de l‚Äôan√†lisi, es pot calcular la bondat d‚Äôajust total d‚Äôuna soluci√≥ $k$-dimensional. Tamb√© es pot calcular la bondat d‚Äôajust per cada fila i columna de la matriu de dades. La bondat d‚Äôajust de les variables tamb√© es pot calcular com l‚Äô$R^2$ en una regressi√≥ sobre les components principals.</p>
:ET